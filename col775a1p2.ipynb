{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8007708,"sourceType":"datasetVersion","datasetId":4716377},{"sourceId":8026939,"sourceType":"datasetVersion","datasetId":4730711},{"sourceId":8027118,"sourceType":"datasetVersion","datasetId":4730843},{"sourceId":8033959,"sourceType":"datasetVersion","datasetId":4735707},{"sourceId":8035579,"sourceType":"datasetVersion","datasetId":4736923}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom nltk.tokenize import word_tokenize,wordpunct_tokenize\nfrom collections import Counter\nimport json\nimport os\nimport torchtext\nfrom collections import Counter,defaultdict\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport os\nimport json\nfrom torchtext.vocab import GloVe\nimport random\nimport gc\nfrom transformers import BertModel, BertTokenizer\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T06:45:13.584421Z","iopub.execute_input":"2024-04-05T06:45:13.585204Z","iopub.status.idle":"2024-04-05T06:45:23.930506Z","shell.execute_reply.started":"2024-04-05T06:45:13.585174Z","shell.execute_reply":"2024-04-05T06:45:23.929671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n# Example text\n# text = [\"Hello, how are you doing today?\",\"(n0)|(n1)#\",'dj']\n\n# Tokenize the text\n# tokens = tokenizer.tokenize(\"(n0)|(n1)#500.2\")\n# print(\"Tokens:\", tokens)\n\n# Convert the tokens to token IDs\n# token_ids = tokenizer.convert_tokens_to_ids(tokens)\n# print(\"Token IDs:\", token_ids)\n\n# Add special tokens and create input IDs and attention mask\n# input_ids = tokenizer.encode(text, add_special_tokens=True)\n# print(\"Input IDs:\", input_ids)\n# tokenized_batch = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n# # Attention mask\n# print(tokenized_batch,'bye')\n# token = tokenizer.convert_ids_to_tokens([102])\n# print(token,'hi')\n# attention_mask = [1] * len(input_ids)\n# print(\"Attention Mask:\", attention_mask)\n# idx_to_word = tokenizer.convert_ids_to_tokens\n# print(idx_to_word(121))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:26.524785Z","iopub.execute_input":"2024-04-05T06:45:26.525723Z","iopub.status.idle":"2024-04-05T06:45:29.916011Z","shell.execute_reply.started":"2024-04-05T06:45:26.525692Z","shell.execute_reply":"2024-04-05T06:45:29.915211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_bert(data):\n#     print(type(data[0]))\n    problem = data['Problem']\n    formula = data['linear_formula']\n    ans = data['answer']\n    max_len_Y = 0\n    tokenized_problem = bert_tokenizer(problem, padding=True, truncation=True, return_tensors=\"pt\")\n    problem = tokenized_problem['input_ids']\n    att_mask = tokenized_problem['attention_mask']\n    for i in range(len(formula)):\n        sentence = formula[i]\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        max_len_Y= max(max_len_Y,len(tokens)+2)\n    for i in range(len(formula)):\n        sentence = formula[i]\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        tokens = ['<start>'] + tokens + ['<end>']\n        padding = ['<pad>']*(max_len_Y-len(tokens))\n        tokens += padding\n        token_idx = []\n        for token in tokens:\n            if(token in word_to_idx_Y):\n                token_idx.append(word_to_idx_Y[token])\n            else:\n                token_idx.append(word_to_idx_Y['<unk>'])\n        formula[i] = token_idx\n    ans = torch.tensor(ans)\n    formula = torch.tensor(formula)\n    return problem,formula,ans,att_mask","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:29.917613Z","iopub.execute_input":"2024-04-05T06:45:29.917909Z","iopub.status.idle":"2024-04-05T06:45:29.928169Z","shell.execute_reply.started":"2024-04-05T06:45:29.917883Z","shell.execute_reply":"2024-04-05T06:45:29.926952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(file_path):\n    with open(file_path+\"/train.json\",'r') as file:\n        train_data=json.load(file)\n    with open(file_path+\"/test.json\",'r') as file:\n        test_data=json.load(file)\n    with open(file_path+\"/dev.json\",'r') as file:\n        val_data=json.load(file)\n    return train_data,val_data,test_data\ndef build_vocab(data,typ):\n    text = [example[typ] for example in data]\n    vocab = Counter()\n    vocab['<pad>'] = 0\n    vocab['<start>'] = 0\n    vocab['<end>'] = 0\n    vocab['<unk>'] = 0\n    max_len = 0\n    for sentence in text:\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        vocab.update(tokens)\n        max_len = max(len(tokens),max_len)\n    idx_to_word = dict(enumerate(vocab))\n    word_to_idx = {word:idx for idx,word in enumerate(vocab)}\n    return vocab,idx_to_word,word_to_idx,max_len\ndef preprocess(data):\n#     print(type(data[0]))\n    problem = data['Problem']\n    formula = data['linear_formula']\n    ans = data['answer']\n    max_len_X = 0\n    max_len_Y = 0\n    for i in range(len(problem)):\n        sentence = problem[i]\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        max_len_X = max(max_len_X,len(tokens)+2)\n    for i in range(len(formula)):\n        sentence = formula[i]\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        max_len_Y= max(max_len_Y,len(tokens)+2)\n    for i in range(len(formula)):\n        sentence = problem[i]\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        tokens = ['<start>'] + tokens + ['<end>']\n        padding = ['<pad>']*(max_len_X-len(tokens))\n        tokens += padding\n        token_idx = []\n        for token in tokens:\n            if(token in word_to_idx_X):\n                token_idx.append(word_to_idx_X[token])\n            else:\n                token_idx.append(word_to_idx_Y['<unk>'])\n        problem[i] = token_idx\n    \n        sentence = formula[i]\n        sentence = sentence.lower()\n        sentence = re.sub(re.escape('|'),' | ',sentence)\n        tokens = word_tokenize(sentence)\n        tokens = ['<start>'] + tokens + ['<end>']\n        padding = ['<pad>']*(max_len_Y-len(tokens))\n        tokens += padding\n        token_idx = []\n        for token in tokens:\n            if(token in word_to_idx_Y):\n                token_idx.append(word_to_idx_Y[token])\n            else:\n                token_idx.append(word_to_idx_Y['<unk>'])\n        formula[i] = token_idx\n    ans = torch.tensor(ans)\n    problem = torch.tensor(problem)\n    formula = torch.tensor(formula)\n    return problem,formula,ans\n# text = preprocess(test_data)\n# print(text)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:29.929526Z","iopub.execute_input":"2024-04-05T06:45:29.929854Z","iopub.status.idle":"2024-04-05T06:45:29.948711Z","shell.execute_reply.started":"2024-04-05T06:45:29.929821Z","shell.execute_reply":"2024-04-05T06:45:29.947824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data,val_data,test_data = load_data('/kaggle/input/text2math/data')\n\ndef data_loader(batch_size):    \n    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    val_dataloader = DataLoader(val_data,batch_size = batch_size,shuffle = False)\n    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n    return train_dataloader,val_dataloader,test_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:29.950710Z","iopub.execute_input":"2024-04-05T06:45:29.951010Z","iopub.status.idle":"2024-04-05T06:45:30.140042Z","shell.execute_reply.started":"2024-04-05T06:45:29.950987Z","shell.execute_reply":"2024-04-05T06:45:30.139231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_X,idx_to_word_X,word_to_idx_X,max_len_X = build_vocab(train_data,'Problem')\nvocab_Y,idx_to_word_Y,word_to_idx_Y,max_len_Y = build_vocab(train_data,'linear_formula')\n# print(vocab_Y)\n# print(idx_to_word_Y)\n# print(word_to_idx_Y)\n# print(len(vocab_X))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:30.141213Z","iopub.execute_input":"2024-04-05T06:45:30.141509Z","iopub.status.idle":"2024-04-05T06:45:44.207963Z","shell.execute_reply.started":"2024-04-05T06:45:30.141483Z","shell.execute_reply":"2024-04-05T06:45:44.207204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GloveEmbeddings():\n    def __init__(self,embedding_dim,word_to_idx):\n        self.embedding_dim = embedding_dim\n        self.word_to_idx = word_to_idx\n        self.vocab_size = len(word_to_idx)\n    def get_embedding(self):\n        glove = GloVe(name = '6B',dim = self.embedding_dim)\n        embedding_matrix = torch.zeros((self.vocab_size,self.embedding_dim))\n        embedding_matrix[0] = torch.zeros(self.embedding_dim) \n        embedding_matrix[1] = torch.randn(self.embedding_dim)\n        embedding_matrix[2] = torch.randn(self.embedding_dim)\n        embedding_matrix[3] = torch.randn(self.embedding_dim)\n        \n        for key,value in self.word_to_idx.items():\n            if value in [0,1,2,3]:\n                continue  \n            else:\n                if key in glove.stoi:\n                    embedding_matrix[value] = torch.tensor(glove.vectors[glove.stoi[key]])\n                else:\n                    embedding_matrix[value] = torch.tensor(embedding_matrix[3])#for unk\n        return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.209155Z","iopub.execute_input":"2024-04-05T06:45:44.209505Z","iopub.status.idle":"2024-04-05T06:45:44.218663Z","shell.execute_reply.started":"2024-04-05T06:45:44.209474Z","shell.execute_reply":"2024-04-05T06:45:44.217815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM_ENCODER(nn.Module):\n    def __init__(self,input_size,embedding_dim,hidden_dim,num_layers,embedding_matrix):\n        super(LSTM_ENCODER,self).__init__()\n        self.input_size = input_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(0.2)\n        self.embedding_matrix = embedding_matrix\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix,padding_idx = 0)\n        self.LSTM = nn.LSTM(self.embedding_dim,self.hidden_dim,num_layers = self.num_layers,bidirectional = True,dropout = 0.2,batch_first = True)\n        self.hidden = nn.Linear(2*self.hidden_dim,self.hidden_dim)\n        self.cell = nn.Linear(2*self.hidden_dim,self.hidden_dim)\n    def forward(self,x,att_mask = None):\n        x = self.dropout(self.embedding(x))\n        out,(h_t,c_t) = self.LSTM(x)\n#         print(x.shape,h_t.shape,c_t.shape)\n#         s = torch.cat((h_t[0:1],h_t[1:2]),dim = 2)\n#         print(s.shape)\n        h_t = self.hidden(torch.cat((h_t[0:1],h_t[1:2]),dim = 2))\n        c_t = self.cell(torch.cat((c_t[0:1],c_t[1:2]),dim = 2))\n#         print(h_t.shape)\n        return out,h_t,c_t\nclass LSTM_DECODER(nn.Module):\n    def __init__(self,input_size,embedding_dim,hidden_dim,num_layers):\n        super(LSTM_DECODER,self).__init__()\n        self.input_size = input_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(0.2)\n        self.embedding = nn.Embedding(self.input_size,self.embedding_dim,padding_idx = 0)\n        self.LSTM = nn.LSTM(self.embedding_dim,self.hidden_dim,num_layers = self.num_layers,dropout = 0.2,batch_first = True)\n        self.fc = nn.Linear(self.hidden_dim,self.input_size)\n    def forward(self,x,h_0,c_0):\n        x = self.dropout(self.embedding(x))\n        x = x.unsqueeze(1)\n        out,(h_t,c_t) = self.LSTM(x,(h_0,c_0))\n        out = self.fc(out)\n        return out,h_t,c_t","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.224684Z","iopub.execute_input":"2024-04-05T06:45:44.224948Z","iopub.status.idle":"2024-04-05T06:45:44.239540Z","shell.execute_reply.started":"2024-04-05T06:45:44.224925Z","shell.execute_reply":"2024-04-05T06:45:44.238726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  \nclass LSTM_Att_DECODER(nn.Module):\n    def __init__(self,input_size,embedding_dim,hidden_dim,num_layers):\n        super(LSTM_Att_DECODER,self).__init__()\n        self.input_size = input_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(0.2)\n        self.embedding = nn.Embedding(self.input_size,self.embedding_dim,padding_idx = 0)\n        self.LSTM = nn.LSTM(2*self.hidden_dim+self.embedding_dim,self.hidden_dim,num_layers = self.num_layers,dropout = 0.2,batch_first = True)#recheck\n        self.att = nn.Linear(3*self.hidden_dim,self.hidden_dim)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim = 1)#recheck\n        self.energy = nn.Linear(self.hidden_dim,1,bias = False)\n        self.fc = nn.Linear(self.hidden_dim,self.input_size)\n    def forward(self,x,encoder_outs,h_t,c_t,att_mask = None):\n        x = self.dropout(self.embedding(x))\n        x = x.unsqueeze(1)\n#         print(h_t.shape,encoder_outs.shape)\n        h_0 = h_t\n        h_t = h_t.repeat(encoder_outs.shape[1],1,1)\n#         print(h_t.shape,\"after\")\n#         print(encoder_outs.shape)\n        h_t = h_t.transpose(0,1)\n        energy = torch.cat([h_t,encoder_outs],dim = 2)\n#         print(energy.shape)\n        energy = self.att(energy)\n        energy = self.relu(energy)\n        energy = self.energy(energy)\n#         energy = self.energy(self.relu(self.att(torch.cat([h_t,encoder_outs],dim = 2))))\n        energy = energy.squeeze(2)\n        \n        attention = self.softmax(energy)\n        attention = attention.unsqueeze(1)\n        print(attention.shape,'hi')\n        context = torch.bmm(attention,encoder_outs)\n#         print(x.shape,context.shape)\n        x = torch.cat([x,context],dim = 2)\n#         print(x.shape)\n        out,(ht,ct) = self.LSTM(x,(h_0,c_t))\n        out = self.fc(out)\n        return out,ht,ct","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.240843Z","iopub.execute_input":"2024-04-05T06:45:44.241147Z","iopub.status.idle":"2024-04-05T06:45:44.254751Z","shell.execute_reply.started":"2024-04-05T06:45:44.241108Z","shell.execute_reply":"2024-04-05T06:45:44.253917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass BERT_ENCODER(nn.Module):\n    def __init__(self,frozen,tune_layers,hidden_units):\n        super(BERT_ENCODER,self).__init__()\n        self.frozen = frozen\n        self.tune_layers = tune_layers\n        self.hidden_units = hidden_units\n        bert_model = BertModel.from_pretrained('bert-base-cased')\n        if self.frozen == True:\n            for name,params in bert_model.named_parameters():\n                params.requires_grad = False\n        else:\n            if self.tune_layers!=-1:\n                for params in bert_model.parameters():\n                    params.requires_grad = False\n                for params in bert_model.encoder.layer[-self.tune_layers:].parameters():\n                    params.requires_grad = True\n        self.encoder = bert_model\n    def forward(self,x,att_mask):\n        outs = self.encoder(input_ids = x,attention_mask = att_mask)\n        encodings = outs.last_hidden_state\n        hidden = None\n        cell = None\n        return encodings,hidden,cell\n    \nclass BERT_Att_DECODER(nn.Module):\n    def __init__(self,input_size,embedding_dim,hidden_dim,num_layers):\n        super(BERT_Att_DECODER,self).__init__()\n        self.input_size = input_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim  \n        self.num_layers = num_layers\n        self.dropout = nn.Dropout(0.2)\n        self.embedding = nn.Embedding(self.input_size,self.embedding_dim,padding_idx = 0)#check\n        self.LSTM = nn.LSTM(self.hidden_dim+self.embedding_dim,self.hidden_dim,num_layers = self.num_layers,dropout = 0.2,batch_first = True)\n        self.att = nn.Linear(2*self.hidden_dim,1)\n#         self.energy = nn.Linear(self.hidden_dim,1,bias = False)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim = 1)\n        self.fc = nn.Linear(self.hidden_dim,self.input_size)\n    def forward(self,x,encoder_outs,h_t,c_t):\n        x = self.dropout(self.embedding(x))\n        x = x.unsqueeze(1)\n#         print(h_t.shape,encoder_outs.shape)\n        h_0 = h_t\n        h_t = h_t.repeat(encoder_outs.shape[1],1,1)\n#         print(h_t.shape,\"after\")\n#         print(encoder_outs.shape)\n        h_t = h_t.transpose(0,1)\n        energy = torch.cat([h_t,encoder_outs],dim = 2)\n#         print(energy.shape)\n        energy = self.att(energy)\n        energy = self.relu(energy)\n#         energy = self.energy(energy)\n#         energy = self.energy(self.relu(self.att(torch.cat([h_t,encoder_outs],dim = 2))))\n        energy = energy.squeeze(2)\n        \n        attention = self.softmax(energy)\n        attention = attention.unsqueeze(1)\n#         print(attention.shape)\n        context = torch.bmm(attention,encoder_outs)\n#         print(x.shape,context.shape)\n        x = torch.cat([x,context],dim = 2)\n#         print(x.shape)\n        out,(ht,ct) = self.LSTM(x,(h_0,c_t))\n        out = self.fc(out)\n        \n        return out,ht,ct","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.256110Z","iopub.execute_input":"2024-04-05T06:45:44.256371Z","iopub.status.idle":"2024-04-05T06:45:44.272197Z","shell.execute_reply.started":"2024-04-05T06:45:44.256348Z","shell.execute_reply":"2024-04-05T06:45:44.271360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self,encoder,decoder,is_att,tf_ratio = 0.6,target_vocab_size = len(vocab_Y)):\n        super(Seq2Seq,self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.tf_ratio = tf_ratio\n        self.target_vocab_size = target_vocab_size\n        self.is_att = is_att\n    def forward(self,problems,formulas,att_mask = None):\n        batch_size = problems.shape[0]\n        target_len = formulas.shape[1]\n        target_vocab_size = self.target_vocab_size   \n        outs,hidden,cell = self.encoder(problems,att_mask)\n        if(cell == None):\n#         outs = self.encoder(problems)\n            hidden = torch.zeros(1, batch_size, self.decoder.hidden_dim).to(device)\n            cell = torch.zeros(1, batch_size,self.decoder.hidden_dim).to(device)\n#         print(target_len)\n        outputs = torch.zeros(batch_size,target_len,target_vocab_size).to(device)\n        x = formulas[:,0]\n#         print(x)\n        for i in range(1,target_len):\n            if(self.is_att):\n                output,hidden,cell = self.decoder(x,outs,hidden,cell)\n            else:\n                output,hidden,cell = self.decoder(x,hidden,cell)\n            output = output.squeeze(1)\n#             print(output.shape)\n            outputs[:,i] = output\n            x = output.argmax(dim = 1)\n            if random.random() < self.tf_ratio:\n                x = formulas[:,i]\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.273200Z","iopub.execute_input":"2024-04-05T06:45:44.273461Z","iopub.status.idle":"2024-04-05T06:45:44.287306Z","shell.execute_reply.started":"2024-04-05T06:45:44.273435Z","shell.execute_reply":"2024-04-05T06:45:44.286393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_encoder_decoder(model_type,input_size,output_size,hidden_dim,embedding_dim,num_layers,embedding_matrix):\n    if(model_type == 'bilstm-lstm'):\n        encoder = LSTM_ENCODER(input_size,embedding_dim,hidden_dim,num_layers,embedding_matrix)\n        decoder = LSTM_DECODER(output_size,embedding_dim,hidden_dim,num_layers)\n    elif(model_type == 'bilstm-att-lstm'):\n        encoder = LSTM_ENCODER(input_size,embedding_dim,hidden_dim,num_layers,embedding_matrix)\n        decoder = LSTM_Att_DECODER(output_size,embedding_dim,hidden_dim,num_layers)\n    elif(model_type == 'frozen-bert-att-lstm'):\n        encoder = BERT_ENCODER(True,-1,hidden_dim)\n        decoder = BERT_Att_DECODER(output_size,embedding_dim,hidden_dim,num_layers)\n    elif(model_type == 'bert-att-lstm'):\n        encoder = BERT_ENCODER(True,1,hidden_dim)\n        decoder = BERT_Att_DECODER(output_size,embedding_dim,hidden_dim,num_layers)\n    return encoder,decoder","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.288493Z","iopub.execute_input":"2024-04-05T06:45:44.288860Z","iopub.status.idle":"2024-04-05T06:45:44.300283Z","shell.execute_reply.started":"2024-04-05T06:45:44.288830Z","shell.execute_reply":"2024-04-05T06:45:44.299344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/val')\nos.makedirs('/kaggle/working/test')\nos.makedirs('/kaggle/working/train')\nos.makedirs('/kaggle/working/model')\nos.makedirs('/kaggle/working/result')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.301214Z","iopub.execute_input":"2024-04-05T06:45:44.301537Z","iopub.status.idle":"2024-04-05T06:45:44.313452Z","shell.execute_reply.started":"2024-04-05T06:45:44.301495Z","shell.execute_reply":"2024-04-05T06:45:44.312701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader,val_loader,test_loader = data_loader(32)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.314646Z","iopub.execute_input":"2024-04-05T06:45:44.314906Z","iopub.status.idle":"2024-04-05T06:45:44.321796Z","shell.execute_reply.started":"2024-04-05T06:45:44.314883Z","shell.execute_reply":"2024-04-05T06:45:44.320972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = len(vocab_X)\noutput_size = len(vocab_Y)\nhidden_dim = 256\nembedding_dim = 300\nnum_layers = 1\nglove = GloveEmbeddings(embedding_dim,word_to_idx_X)\nembedding_matrix = glove.get_embedding()\nmax_len_X = max_len_X+2\nmax_len_Y = max_len_Y+2","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:45:44.322938Z","iopub.execute_input":"2024-04-05T06:45:44.323276Z","iopub.status.idle":"2024-04-05T06:49:59.781737Z","shell.execute_reply.started":"2024-04-05T06:45:44.323246Z","shell.execute_reply":"2024-04-05T06:49:59.780877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda\"\n    print('using device: cuda')\nelse:\n    device = \"cpu\"\n    print('using device: cpu')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.785660Z","iopub.execute_input":"2024-04-05T06:49:59.785940Z","iopub.status.idle":"2024-04-05T06:49:59.813776Z","shell.execute_reply.started":"2024-04-05T06:49:59.785916Z","shell.execute_reply":"2024-04-05T06:49:59.812862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef train():\n    models = ['bilstm-lstm','bilstm-att-lstm','frozen-bert-att-lstm','bert-att-lstm']\n    encoder,decoder = get_encoder_decoder(models[1],input_size,output_size,hidden_dim,embedding_dim,num_layers,embedding_matrix)\n    model = Seq2Seq(encoder,decoder,True)\n#     model = nn.DataParallel(model)\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n    \n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,50,verbose = False)\n    \n    loss_dict = defaultdict(list)\n    accuracy_dict = defaultdict(list)\n    \n    best_epoch = 0\n    least_loss = 1e9+7\n    correct = 0\n    correct_token = 0\n    print('-----------------------------------Training------------------------------------')\n    for epoch in range(50):\n        model.train()\n        print(f\"\\n----------------------------epoch: {epoch}----------------------------------\")\n        train_loss = []\n        for idx,batch in enumerate(train_loader):\n            optimizer.zero_grad()\n#             print(batch)\n            problems,formulas,ans = preprocess(batch)\n            \n            problems = problems.to(device)\n            formulas = formulas.to(device)\n#             att_mask = att_mask.to(device)\n            outputs = model(problems,formulas)\n            \n            outputs = outputs.reshape(-1,outputs.shape[2])\n            \n            formulas = formulas.reshape(-1)\n            \n            loss = criterion(outputs,formulas)\n            loss.backward()\n            \n            train_loss.append(loss.item())\n            \n            optimizer.step()\n            # Assuming `obj` is the object consuming GPU memory\n            obj = None\n\n# Collect garbage\n            gc.collect()\n\n# Empty PyTorch cache\n            torch.cuda.empty_cache()\n#             _, predicted = torch.max(outputs, 2)\n            # count correct if exact match\n#             for pred, target in zip(predicted, formulas):\n#                 if torch.equal(pred, target):\n#                     correct += 1\n#             total += formulas.size(0)\n#             # count correct tokens\n#             for pred, target in zip(predicted, targets):\n#                 for p, t in zip(pred, target):\n#                     if p == t:\n#                         correct_token += 1\n#                     total_token += 1\n            \n        scheduler.step()\n#         accuracy = correct / total\n        model.eval()\n        \n        val_loss = []\n        print('-------------------------------------Validation---------------------------')\n        for idx,batch in enumerate(val_loader):\n            \n            problems,formulas,ans = preprocess(batch)\n            \n            problems = problems.to(device)\n            formulas = formulas.to(device)\n#             att_mask = att_msk.to(device)\n            output = model(problems,formulas)\n            \n            output = output.reshape(-1,output.shape[2])\n            \n            formulas = formulas.reshape(-1)\n            \n            loss = criterion(output,formulas)\n            \n            val_loss.append(loss.item())\n            \n        loss_val = np.mean(val_loss)\n        loss_train = np.mean(train_loss)\n        \n        loss_dict['train'].append(loss_train)\n        loss_dict['val'].append(loss_val)\n            \n        print(f\"epoch:{epoch}, Train_loss:{loss_train}, Val_loss: {loss_val}\")\n        \n        print('----saving checkpoint-----')\n        with open(os.path.join('/kaggle/working/train','loss_dict_lstm-att3-lstm.json'),'w') as outfile:\n            json.dump(loss_dict,outfile)\n        torch.save(model,os.path.join('/kaggle/working/model','latest_model_lstm-att3-lstm.pth'))\n        \n        if(loss_val<least_loss):\n            print(f\"-------------updating least loss{loss_val} against{least_loss}-------------\")\n            least_loss = loss_val\n            print('-----saving best model-----------')\n            torch.save(model,os.path.join('/kaggle/working/model','best_model_lstm-att3-lstm.pth'))\n    return \n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.815481Z","iopub.execute_input":"2024-04-05T06:49:59.815799Z","iopub.status.idle":"2024-04-05T06:49:59.834299Z","shell.execute_reply.started":"2024-04-05T06:49:59.815773Z","shell.execute_reply":"2024-04-05T06:49:59.833445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.835502Z","iopub.execute_input":"2024-04-05T06:49:59.835826Z","iopub.status.idle":"2024-04-05T06:49:59.850154Z","shell.execute_reply.started":"2024-04-05T06:49:59.835802Z","shell.execute_reply":"2024-04-05T06:49:59.849234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = torch.load('/kaggle/input/latest-lstm-att-tf3/latest_model_lstm-att-lstm3.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.851433Z","iopub.execute_input":"2024-04-05T06:49:59.852048Z","iopub.status.idle":"2024-04-05T06:49:59.858881Z","shell.execute_reply.started":"2024-04-05T06:49:59.852015Z","shell.execute_reply":"2024-04-05T06:49:59.858051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Assuming `obj` is the object consuming GPU memory\n# obj = None\n\n# # Collect garbage\n# gc.collect()\n\n# # Empty PyTorch cache\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.859978Z","iopub.execute_input":"2024-04-05T06:49:59.860254Z","iopub.status.idle":"2024-04-05T06:49:59.871833Z","shell.execute_reply.started":"2024-04-05T06:49:59.860231Z","shell.execute_reply":"2024-04-05T06:49:59.871030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def beam_search(model,encoder_outs, enc_hidden, enc_cell, max_tgt_length=411, beam_size=10,att = True):\n    beam = [([1], (enc_hidden, enc_cell), 0)]\n    step = 0\n    print('hi')\n    while step < max_tgt_length - 1:\n        new_beam = []\n        for seq, (hidden, cell), score in beam:\n            prev_token = [seq[-1]]\n            prev_token = torch.LongTensor(prev_token).to(device)\n#             print(hidden.shape,'hi')\n            if(att):\n                decoder_output, hidden, cell = model.decoder(prev_token,encoder_outs,hidden, cell)\n            else:\n                decoder_output, hidden, cell = model.decoder(prev_token, hidden, cell)\n            decoder_output = decoder_output.squeeze(1)\n            \n            top_info = decoder_output.topk(beam_size, dim=1)\n            top_values, top_indices = top_info\n            \n            for j in range(beam_size):\n                new_word_index = top_indices[0][j]\n                new_seq = seq + [new_word_index.item()]\n                new_word_prob = torch.log(top_values[0][j])\n                updated_score = score - new_word_prob\n                new_candidate = (new_seq, (hidden, cell), updated_score)\n                new_beam.append(new_candidate)\n        new_beam.sort(key=lambda x: x[2])\n        beam = new_beam[:beam_size]\n        step += 1\n#         print(beam)\n    best_candidate = beam[0][0]\n    decoded_words = torch.zeros(1, max_tgt_length)\n#         print(best_candidate)\n    for t in range(max_tgt_length):\n        decoded_words[:, t] = torch.LongTensor([best_candidate[t]])\n    return decoded_words\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.873161Z","iopub.execute_input":"2024-04-05T06:49:59.873463Z","iopub.status.idle":"2024-04-05T06:49:59.885255Z","shell.execute_reply.started":"2024-04-05T06:49:59.873417Z","shell.execute_reply":"2024-04-05T06:49:59.884226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def idx_to_sentence(output,idx_to_word):\n    output = output.cpu().detach().numpy()\n    preds = []\n    for i in range(len(output)):\n        p = []\n        for idx in output[i]:\n            w = idx_to_word[idx]\n            if(w == \"<end>\"):\n                break\n            else:\n                p.append(w)\n        preds.append(\"\".join(p[1:]))\n#     preds = torch.tensor(preds)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.886404Z","iopub.execute_input":"2024-04-05T06:49:59.886720Z","iopub.status.idle":"2024-04-05T06:49:59.895452Z","shell.execute_reply.started":"2024-04-05T06:49:59.886696Z","shell.execute_reply":"2024-04-05T06:49:59.894572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_eval(model,idx_to_word,loader,m_id):\n#     models = ['bilstm-lstm','bilstm-att-lstm','frozen-bert-att-lstm','bert-att-lstm']\n#     m_id = 0\n    beam = True\n    results = []\n    target_vocab_size = len(idx_to_word)\n    outfile = []\n    for idx,batch in enumerate(loader):\n#         for i in range(len(batch)):\n#             outfile.append({\"Problem\":batch[\"Problem\"][i],\"linear_formula\":batch[\"linear_formula\"][i],\"answer\":batch[\"answer\"][i],\"predicted\":0})\n        problems,formulas,ans = preprocess(batch)\n        problems = problems.to(device)\n        formulas = formulas.to(device)\n#         att_mask = att_mask.to(device)\n#         print(formulas.shape,'f')\n        batch_size = problems.shape[0]\n        max_target_len = formulas.shape[1]\n        \n        preds = torch.zeros(batch_size,max_target_len).to(device)\n        encoder_outs,hidden,cell = model.encoder(problems)\n#         print(hidden.shape,'hiii')\n#         print(cell.shape,'byee')\n#         print(encoder_outs.shape,'encodershape')\n        if(cell == None):\n#         outs = self.encoder(problems)\n            hidden = torch.zeros(1, batch_size, model.decoder.hidden_dim).to(device)\n            cell = torch.zeros(1, batch_size,model.decoder.hidden_dim).to(device)\n        if(beam):\n            for b in range(batch_size):\n                if(m_id<2):\n                    preds[b,:] = beam_search(model,encoder_outs[b,:,:].unsqueeze(0),hidden[:,b,:].unsqueeze(1),cell[:,b,:].unsqueeze(1),max_target_len,beam_size = 10,att = False)\n                else:\n                    preds[b,:] = beam_search(model,encoder_outs[b,:,:].unsqueeze(0),hidden[:,b,:].unsqueeze(1),cell[:,b,:].unsqueeze(1),max_target_len,beam_size = 1,att = True)\n                    \n        else:\n            outputs = torch.zeros(batch_size,max_target_len,target_vocab_size).to(device)\n            preds = torch.zeros(batch_size,max_target_len).to(device)\n            x = formulas[:,0]\n            preds[:,0] = formulas[:,0]\n#             print(formulas[:,0])\n            for t in range(1,max_target_len):\n                output,hidden,cell = model.decoder(x,hidden,cell)\n                output = output.squeeze(1)\n                outputs[:,t,:] = output\n                x = output.argmax(dim = 1)\n                preds[:,t] = x\n#             print(preds)\n        predictions = idx_to_sentence(preds,idx_to_word)\n#         for i in range(len(batch)):\n#             outfile[i][\"predicted\"] = predictions[i]\n        results.append(predictions)\n        \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.896640Z","iopub.execute_input":"2024-04-05T06:49:59.896934Z","iopub.status.idle":"2024-04-05T06:49:59.911686Z","shell.execute_reply.started":"2024-04-05T06:49:59.896905Z","shell.execute_reply":"2024-04-05T06:49:59.910793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = model_eval(model,idx_to_word_Y,1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.913377Z","iopub.execute_input":"2024-04-05T06:49:59.913747Z","iopub.status.idle":"2024-04-05T06:49:59.924994Z","shell.execute_reply.started":"2024-04-05T06:49:59.913717Z","shell.execute_reply":"2024-04-05T06:49:59.924002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx,batch in enumerate(train_loader):\n#     print(batch['linear_formula'][5])\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.926257Z","iopub.execute_input":"2024-04-05T06:49:59.926882Z","iopub.status.idle":"2024-04-05T06:49:59.934536Z","shell.execute_reply.started":"2024-04-05T06:49:59.926851Z","shell.execute_reply":"2024-04-05T06:49:59.933658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(type(results))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.935704Z","iopub.execute_input":"2024-04-05T06:49:59.935979Z","iopub.status.idle":"2024-04-05T06:49:59.943936Z","shell.execute_reply.started":"2024-04-05T06:49:59.935956Z","shell.execute_reply":"2024-04-05T06:49:59.943209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_outfile(model_file,loader,m_id):\n    outfile = []\n#     torch.backends.cuda.matmul.allow_tf32 = False  # Disables TF32 to improve compatibility\n#     torch.backends.cudnn.benchmark = False  # Disable CuDNN benchmark mode\n#     torch.backends.cudnn.deterministic = True\n    model = torch.load(model_file)\n        # Then move the model to the appropriate device\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n#     print(model)\n    results = model_eval(model,idx_to_word_Y,loader,m_id)\n    for idx,batch in enumerate(loader):\n        for i in range(len(batch)):\n            problem = batch[\"Problem\"][i]\n            linear_formula = batch[\"linear_formula\"][i]\n            answer = batch[\"answer\"][i]\n            predicted = results[idx][i]  # Convert to NumPy array and then to list if necessary\n#             print(type(problem),type(linear_formula),type(answer),type(predicted))\n            # Append serializable data to outfile\n            answer = int(answer)\n            outfile.append({\"Problem\": problem, \"linear_formula\": linear_formula, \"answer\": answer, \"predicted\": predicted})\n#             outfile.append({\"Problem\":batch[\"Problem\"][i],\"linear_formula\":batch[\"linear_formula\"][i],\"answer\":batch[\"answer\"][i],\"predicted\":results[idx][i]})\n#     print(outfile)\n    with open(os.path.join('/kaggle/working/result','results_lstm_test_beam.json'),'w') as file:\n            json.dump(outfile,file)\n    return outfile","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.945046Z","iopub.execute_input":"2024-04-05T06:49:59.945403Z","iopub.status.idle":"2024-04-05T06:49:59.954918Z","shell.execute_reply.started":"2024-04-05T06:49:59.945366Z","shell.execute_reply":"2024-04-05T06:49:59.954009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(results[0][5])\nprint('hi')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:49:59.956106Z","iopub.execute_input":"2024-04-05T06:49:59.956375Z","iopub.status.idle":"2024-04-05T06:49:59.968067Z","shell.execute_reply.started":"2024-04-05T06:49:59.956354Z","shell.execute_reply":"2024-04-05T06:49:59.967161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outfile = create_outfile('/kaggle/input/latest-model-lstm/latest_model_lstm-lstm.pth',test_loader,1)\n# print(outfile)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T07:02:34.671551Z","iopub.execute_input":"2024-04-05T07:02:34.671904Z","iopub.status.idle":"2024-04-05T07:02:38.301388Z","shell.execute_reply.started":"2024-04-05T07:02:34.671877Z","shell.execute_reply":"2024-04-05T07:02:38.300104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(outfile)\n# TORCH_USE_CUDA_DSA\n# ","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:22:35.031670Z","iopub.execute_input":"2024-04-04T19:22:35.032545Z","iopub.status.idle":"2024-04-04T19:22:35.076837Z","shell.execute_reply.started":"2024-04-04T19:22:35.032501Z","shell.execute_reply":"2024-04-04T19:22:35.075259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json \n# import sys \n# import re \n# import math\n# import pickle\n# LPAR = '('\n# RPAR = ')'\n# ADD = 'add'\n# SUB = 'subtract'\n# DIV = 'divide'\n# POW = 'power'\n# MUL = 'multiply'\n# NEG = 'negate'\n# CAREA = 'circle_area'\n# SAREA = 'square_area'\n# RAREA = 'rectangle_area'\n# VCYL = 'volume_cylinder'\n# CONST = 'const_'\n# FLOOR = 'floor'\n# SQRT = 'sqrt'\n# CIRCUM = 'circumface'\n# INV = 'inverse'\n# FACT = 'factorial'\n# LOG = 'log'\n# CHOOSE = 'choose'\n# RHPERI = 'rhombus_perimeter'\n# RPERI = 'rectangle_perimeter'\n# QAREA = 'quadrilateral_area'\n# SPEED = 'speed'\n# MOD = 'reminder'\n# VRPR = 'volume_rectangular_prism'\n# PERMUT = 'permutation'\n# ASPHERE = 'surface_sphere'\n# TAREA = 'triangle_area'\n# GCD = 'gcd'\n# LCM = 'lcm'\n# TPERI = 'triangle_perimeter'\n# PGAIN = 'p_after_gain'\n# TAREA3 =  'triangle_area_three_edges'\n# CUBE_EDGE = 'cube_edge_by_volume'\n# SRPR = 'surface_rectangular_prism'\n# SQPERI = 'square_perimeter'\n# MAX = 'max'\n# SRCU = 'surface_cube'\n# VCU = 'volume_cube'\n# RHAREA =  'rhombus_area'\n# OPRBLOSS = 'original_price_before_loss'\n# SQUAE_EDGE =  'square_edge_by_area'\n# VCONE = 'volume_cone'\n# SS = 'stream_speed'\n# SURF_CYL = 'surface_cylinder'\n# VSPHERE = 'volume_sphere'\n# OPRBGAIN =  'original_price_before_gain'\n# MIN = 'min'\n# SQR_EDGE_BY_PERI = 'square_edge_by_perimeter'\n# NEG_PROB =  'negate_prob'\n\n# #factorial function\n# def factorial(n):\n#     ans = 1\n#     for i in range(1, int(n+1)):\n#         ans *= i\n#     return ans \n\n# #lcm function\n# def lcm(a, b):\n#     return abs(a * b) // math.gcd(a, b)\n\n# #area of triangle\n# def triangle_area(a, b, c):\n#     # Calculate the semi-perimeter\n#     s = (a + b + c) / 2\n    \n#     # Calculate the area using Heron's formula\n#     area = math.sqrt(s * (s - a) * (s - b) * (s - c))\n#     return area\n\n# #the huge operation dict\n# ops = {\n\n#     ADD: lambda x,y: x+y,\n#     SUB: lambda x,y: y-x,\n#     DIV: lambda x,y: y/x,\n#     POW: lambda x,y: y**x,\n#     MUL: lambda x,y: x*y,\n#     NEG: lambda x: -1*x,\n#     CAREA: lambda x: (x**2)*math.pi,\n#     VCYL: lambda x,y: 1/3*math.pi*(x**2)*y,\n#     FLOOR: lambda x: int(x),\n#     SQRT: lambda x: (x**0.5),\n#     CIRCUM: lambda x: 2*math.pi*x,\n#     SAREA: lambda x: x**2,\n#     INV: lambda x: 1/x,\n#     RAREA: lambda x,y: x*y,\n#     FACT: factorial,\n#     LOG: lambda x: math.log(x),\n#     CHOOSE: lambda x,y: factorial(y)/(factorial(x)*factorial(y-x)),\n#     RPERI: lambda x,y: 2*(x+y),\n#     QAREA: lambda x,y,z: (y+x)/2*z, #new 15141\n#     SPEED: lambda x,y: y/x,\n#     MOD: lambda x,y: int(y)%int(x),\n#     VRPR: lambda x,y,z: y*z*x,\n#     RHPERI: lambda x: 4*x,\n#     PERMUT: lambda x,y: factorial(y)/factorial(x),\n#     ASPHERE: lambda x: 4*math.pi*(x**2),\n#     TAREA: lambda x,y: (x*y)/2,\n#     GCD: lambda x,y: math.gcd(int(y),int(x)),\n#     LCM: lambda x,y: lcm(int(y), int(x)),\n#     TPERI: lambda x,y,z: (x+y+z),\n#     PGAIN: lambda x,y: y/x,\n#     TAREA3: lambda x,y,z: triangle_area(x,y,z),\n#     CUBE_EDGE: lambda x: x**(1/3),\n#     SRPR: lambda x,y,z: 2*(x*y + y*z + z*x),\n#     SQPERI: lambda x: 4*x,\n#     MAX: lambda x,y: max(x,y),\n#     SRCU: lambda x: 6*(x**2),\n#     VCU: lambda x: 6*(x**2),\n#     RHAREA: lambda x,y: (x+y)/2,\n#     OPRBLOSS: lambda x,y: (100 - y)/100*x,\n#     SQUAE_EDGE: lambda x: (x**0.5),\n#     VCONE: lambda x,y: math.pi*(y**2)/3*y,\n#     SS: lambda x,y: (x+y)/2,\n#     SURF_CYL: lambda x,y: math.pi*2*x*y + 2*math.pi*(y**2),\n#     VSPHERE: lambda x: 4/3*math.pi*(x**3),\n#     OPRBGAIN: lambda x,y: (100+y)/100*x,\n#     MIN: lambda x,y: min(x,y),\n#     SQR_EDGE_BY_PERI: lambda x: x/4,\n#     NEG_PROB: lambda x: (1-x)\n\n# }\n\n# #unitary operators\n# uni_op = [NEG, SQRT, FLOOR, CAREA, CIRCUM, SAREA, INV, \n#             FACT, LOG, RHPERI, ASPHERE, CUBE_EDGE, SQPERI,\n#             SRCU, VCU, SQUAE_EDGE, VSPHERE, \n#             SQR_EDGE_BY_PERI, NEG_PROB]\n\n# #teranary operator\n# tri_op = [QAREA, VRPR, TPERI, TAREA3, SRPR]\n\n# #solve for one op\n# def solve(op, args):\n    \n#     a1 = args[0]\n#     #unary ?\n#     if(op in uni_op):\n#         ans = ops[op](a1)\n    \n#     #trinary operator\n#     elif(op in tri_op):\n#         a2 = args[1]\n#         a3 = args[2]\n#         ans = ops[op](a3, a2, a1)\n\n#     #binary\n#     else:\n#         a2 = args[1]\n#         ans = ops[op](a2, a1)\n#     return ans\n\n# #remove the commas\n# pattern = r'\\b\\d{1,3}(,\\d{3})+\\b'\n\n# # Function to remove commas from matched integers\n# def remove_commas(match):\n#     return match.group(0).replace(',', '')\n\n# #get n0, n1, .. from problem statement\n# def get_nums(sent):\n\n#     #remove commas\n#     global pattern\n#     sent = re.sub(pattern, remove_commas, sent)\n\n#     #match the digits\n#     pt = r'[-+]?\\d*\\.\\d+|\\d+'\n    \n#     #sent = re.sub(sent, remove_commas, sent)\n#     matches = re.findall(pt, sent)\n#     numbers = [float(match) for match in matches]\n#     return numbers\n\n# #resolve the args\n# def resolve_args(argsi, ans, nums):\n\n#     #prev answers\n#     if('#' in argsi):\n#         argsi = argsi.split('#')[1].strip()\n#         ret = ans[int(argsi)]\n\n#     #some constant\n#     elif('const_' in argsi):\n#         argsi = argsi.strip()[6:]\n#         if(argsi == 'pi'):\n#             ret = math.pi \n#         else:\n#             argsi = argsi.replace('_', '.')\n#             ret = float(argsi)\n\n#     #are they from input sentence\n#     elif('n' in argsi):\n#         w = int(argsi.strip()[1:])\n#         ret = nums[w]\n    \n#     return ret \n\n# #evaluate linear formula\n# def eval(x, sent):\n    \n#     #get number from sentences\n#     nums = get_nums(sent)\n    \n#     #divide in the operations\n#     ops = x.split('|')\n#     ans = []\n\n#     #iterate for every options\n#     for oi in ops:\n\n#         if(oi == ''):\n#             continue\n\n#         #split on (\n#         t = oi.split('(')\n        \n#         #the operation\n#         op = t[0]\n        \n#         #get the arg\n#         args = t[1].split(')')[0]\n#         args = args.split(',')\n#         args = [argsi.strip() for argsi in args]\n\n#         #resolve the args\n#         #print(op, args)\n#         args = [resolve_args(argsi, ans, nums) for argsi in args]\n\n#         #get the ans\n#         ansi = solve(op, args)\n#         ans.append(ansi)\n\n#     return ans[-1]\n\n# #evaluate the linear expression and add to json\n# def transform(data):\n\n#     right = 0\n#     wrong = 0\n#     exact_match = 0\n#     tr_data = []\n    \n#     for data_i in data:\n        \n#         #user predcition may not be executable\n#         try: \n            \n#             #copy data\n#             tr_data_i = data_i\n\n#             #the exact match acc\n#             exact_match += (tr_data_i['predicted'] == tr_data_i['linear_formula'])\n\n#             #evaluate\n#             tr_data_i['predicted_answer'] = eval(data_i['predicted'], data_i['Problem'])\n\n#             #print(data_i['annotated_formula'])\n#             err = abs((tr_data_i['predicted_answer'] - tr_data_i['answer'] )/ tr_data_i['answer'])\n            \n#             #if beyond 2% then wrong\n#             if(err > 0.02 or type(tr_data_i['predicted_answer']) == complex):\n#                 wrong += 1\n#                 tr_data_i['predicted_answer'] = None\n\n#             #else match\n#             else:\n#                 right += 1\n        \n#             #return \n#             tr_data.append(tr_data_i)\n#         except: \n            \n#             tr_data_i['predicted_answer'] = None\n#             wrong += 1\n#             tr_data.append(tr_data_i)\n\n#     print(f\"Execution Accuracy: {right / (right + wrong)*100}!!\" )\n#     print(f\"Exact Match Accuracy: {exact_match / (right + wrong)*100}!!\" )\n#     return tr_data\n\n# def main(file):\n\n#     f = open(file, 'rb')\n#     data = json.load(f)\n\n#     #convert the data\n#     tr_data = transform(data)\n\n#     #write back to file\n#     f = open(file, 'w')\n#     json.dump(tr_data, f, indent='\\t', separators=(',', ': '))\n# main(\"/kaggle/working/result/results_att_val_6.json\")\n\n\n# '''\n# For the input json entry :\n\n#     {\n# \t\t\"Problem\": \"a multiple choice test consists of 4 questions , and each question has 5 answer choices . in how many r ways can the test be completed if every question is unanswered ?\",\n# \t\t\"answer\": 625,\n#         \"linear_formula\": \"power(n1,n0)|\"\n# \t},\n\n# The output json file entry should contain your prediction in predicted key\n# and all other input fields:\n\n#     {\n# \t\t\"Problem\": \"a multiple choice test consists of 4 questions , and each question has 5 answer choices . in how many r ways can the test be completed if every question is unanswered ?\",\n# \t\t\"answer\": 625,\n#         \"predicted\": \"power(n0,n0)|\",\n#         \"linear_formula\": \"power(n1,n0)|\"\n# \t},\n\n# We are counting on you to maintain the same files\n\n# This script will write the predicted output back to output file\n\n\n#     {\n# \t\t\"Problem\": \"a multiple choice test consists of 4 questions , and each question has 5 answer choices . in how many r ways can the test be completed if every question is unanswered ?\",\n# \t\t\"answer\": 625,\n#         \"predicted\": \"power(n0,n0)|\",\n#         \"linear_formula\": \"power(n1,n0)|\",\n#         \"predicted_answer\": 256\n# \t},\n# '''","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:21:13.635559Z","iopub.status.idle":"2024-04-04T19:21:13.636026Z","shell.execute_reply.started":"2024-04-04T19:21:13.635822Z","shell.execute_reply":"2024-04-04T19:21:13.635840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}